import json
import logging
import os
import time
import re
from flask import Flask, request, jsonify
from google.cloud import firestore
from openai import OpenAI

app = Flask(__name__)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("app.log", encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# Firestore í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    db = firestore.Client()
    logger.info("Firestore client initialized")
except Exception as e:
    logger.error(f"Failed to initialize Firestore: {e}")
    db = None

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    logger.info("OpenAI client initialized")
except Exception as e:
    logger.error(f"Failed to initialize OpenAI: {e}")
    openai_client = None

# ì¼ë³¸ì–´ ê°ì§€ í•¨ìˆ˜
def needs_translation(title: str) -> bool:
    if not title or not isinstance(title, str):
        logger.debug(f"No translation needed: title is empty or invalid: {title}")
        return False
    # íˆë¼ê°€ë‚˜(\u3040-\u309F), ê°€íƒ€ì¹´ë‚˜(\u30A0-\u30FF), í•œì(\u4E00-\u9FFF) í¬í•¨ ì—¬ë¶€ í™•ì¸
    has_japanese = bool(re.search(r'[\u3040-\u30FF\u4E00-\u9FFF]', title))
    logger.debug(f"Japanese detection for '{title}': {'Detected' if has_japanese else 'Not detected'}")
    return has_japanese

# Firestore ìºì‹œ í™•ì¸
def get_cached_data(platform, identifier):
    if not db:
        return None
    try:
        doc_ref = db.collection('games').document(platform).collection('items').document(identifier)
        doc = doc_ref.get()
        if doc.exists:
            data = doc.to_dict()
            logger.debug(f"Cache hit for {platform}:{identifier}, title_kr={data.get('title_kr')}")
            return data
        logger.debug(f"Cache miss for {platform}:{identifier}")
        return None
    except Exception as e:
        logger.error(f"Firestore cache error for {platform}:{identifier}: {e}")
        return None

def cache_data(platform, rj_code, data):
    doc_id = f"{platform}_{rj_code}"
    try:
        db.collection("games").document(doc_id).set(data, merge=True)
        logger.info(f"[CACHE] ì €ì¥ë¨: {doc_id}")
    except Exception as e:
        logger.error(f"[CACHE ERROR] ì €ì¥ ì‹¤íŒ¨: {doc_id}, error={e}", exc_info=True)


# íƒœê·¸ ìºì‹œ
def get_cached_tag(tag_jp):
    if not db:
        return None
    try:
        safe_tag_id = normalize_tag_id(tag_jp)
        doc_ref = db.collection('tags').document('jp_to_kr').collection('mappings').document(safe_tag_id)
        doc = doc_ref.get()
        if doc.exists:
            return doc.to_dict()
        return None
    except Exception as e:
        logger.error(f"Tag cache error for {tag_jp}: {e}")
        return None


def normalize_tag_id(tag_jp):
    # Firestore ë¬¸ì„œ IDë¡œ ì“¸ ìˆ˜ ìˆë„ë¡ ìŠ¬ë˜ì‹œ ì œê±° ë˜ëŠ” ëŒ€ì²´
    return tag_jp.replace("/", "__")
def cache_tag(tag_jp, tag_kr, priority):
    if not db:
        return
    try:
        safe_tag_id = normalize_tag_id(tag_jp)
        doc_ref = db.collection('tags').document('jp_to_kr').collection('mappings').document(safe_tag_id)
        doc_ref.set({
            'tag_jp': tag_jp,        # ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥
            'tag_kr': tag_kr,
            'priority': priority
        })
        logger.info(f"Cached tag: {tag_jp} â†’ {tag_kr} (id: {safe_tag_id})")
    except Exception as e:
        logger.error(f"Tag cache error for {tag_jp}: {e}")

# GPT ë²ˆì—­
def translate_with_gpt_batch(tags, title_jp=None, batch_idx=""):
    if not openai_client:
        logger.warning("OpenAI client not initialized")
        return tags, title_jp
    try:
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        prompt = (
            "ë‹¹ì‹ ì€ ì¼ë³¸ì–´ì—ì„œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ì˜ íƒœê·¸ë“¤ê³¼ ì œëª©ì„ ë¬¸ë§¥ì— ë§ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë©°, ì˜ˆì‹œì™€ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì œëª©(title)ë²ˆì—­ì€ í•œêµ­ì–´ë‚˜ ì˜ì–´ì¸ ê²½ìš°ë§Œ ìƒëµí•´ë„ ë©ë‹ˆë‹¤.\n\n"
            "ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\n"
            "{\n"
            "  \"tags\": [\"ë²ˆì—­ëœíƒœê·¸1\", \"ë²ˆì—­ëœíƒœê·¸2\", ...],\n"
            "  \"title\": \"ë²ˆì—­ëœì œëª©\"\n"
            "}\n\n"
            "ì˜ˆì‹œ ì…ë ¥/ì¶œë ¥:\n"
            "Input:\n"
            "Tags: RPG, ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n"
            "Title: å°‘å¥³ã®å†’é™º\n"
            "Output:\n"
            "{\n"
            "  \"tags\": [\"RPG\", \"ì•¡ì…˜\"],\n"
            "  \"title\": \"ì†Œë…€ì˜ ëª¨í—˜\"\n"
            "}\n\n"
            f"Input:\n"
            f"Tags: {', '.join(tags) if tags else 'ì—†ìŒ'}\n"
            f"Title: {title_jp if title_jp else 'ì—†ìŒ'}\n"
            "Output:"
        )

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a translator specializing in Japanese to Korean."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200
        )
        response_text = response.choices[0].message.content.strip()
        logger.debug(f"GPT response for {batch_idx}: {response_text}")

        # JSON íŒŒì‹±
        try:
            response_json = json.loads(response_text)
            translated_tags = response_json.get('tags', tags)
            translated_title = response_json.get('title', title_jp) if title_jp else title_jp
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON response for {batch_idx}: {response_text}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ íŒŒì‹±
            parts = response_text.split(';')
            translated_tags = [t.strip() for t in parts[0].split(',')][:len(tags)]
            translated_title = parts[1].strip() if len(parts) > 1 and title_jp else title_jp

        # ë²ˆì—­ ì‹¤íŒ¨ ê°ì§€
        if title_jp and translated_title and needs_translation(translated_title):
            logger.warning(f"Translation failed for {batch_idx}: translated_title={translated_title} is still Japanese")
            translated_title = title_jp  # ì¼ë³¸ì–´ë¡œ ë°˜í™˜ëœ ê²½ìš° ì›ë³¸ ìœ ì§€

        logger.info(f"Translated for {batch_idx}: tags={translated_tags}, title={translated_title}")
        return translated_tags, translated_title
    except Exception as e:
        logger.error(f"GPT translation error for batch {batch_idx}: {e}")
        return tags, title_jp  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë˜ ì œëª© ìœ ì§€

# RJ ë°ì´í„° ì²˜ë¦¬
def process_rj_item(item):
    if 'error' in item:
        rj_code = item.get('rj_code') or 'invalid'
        error_data = {
            'rj_code': rj_code,
            'error': item.get('error'),
            'platform': 'rj',
            'timestamp': time.time()
        }
        cache_data('rj', rj_code, error_data)
        return error_data



    rj_code = item.get('rj_code')
    cached = get_cached_data('rj', rj_code)
    if cached and cached.get('title_kr') and not needs_translation(cached.get('title_kr')):
        logger.debug(f"Using cached data for {rj_code}: title_kr={cached.get('title_kr')}")
        return cached

    tags_jp = item.get('tags_jp', [])
    title_jp = item.get('title_jp', '')
    tags_kr = []
    tags_to_translate = []
    tag_priorities = []

    for tag in tags_jp:
        cached_tag = get_cached_tag(tag)
        if cached_tag:
            kr = cached_tag['tag_kr']
            priority = cached_tag.get('priority', 10)
            tags_kr.append(kr)
            tag_priorities.append(priority)
        else:
            tags_to_translate.append(tag)
            tag_priorities.append(10)  # ê¸°ë³¸ê°’ ì„¤ì • (ìˆœì„œ ìœ ì§€ ìœ„í•´)

    translated_tags = tags_jp
    translated_title = title_jp

    if needs_translation(title_jp) or tags_to_translate:
        logger.debug(f"Translating for {rj_code}: title_jp={title_jp}, tags={tags_to_translate}")
        translated_tags, translated_title = translate_with_gpt_batch(
            tags_to_translate,
            title_jp if needs_translation(title_jp) else None,
            batch_idx=rj_code
        )
        for i, (jp, kr) in enumerate(zip(tags_to_translate, translated_tags)):
            # Firestoreì—ì„œ ì„¤ì •ëœ priority ë¨¼ì € ì¡°íšŒ
            existing = get_cached_tag(jp)
            priority = existing.get("priority", 10) if existing else 10
            tags_kr.append(kr)
            tag_priorities.append(priority)
            cache_tag(jp, kr, priority)
    else:
        logger.debug(f"No translation needed for {rj_code}: title_jp={title_jp}")

    # âœ… priority ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    tag_with_priority = list(zip(tags_kr, tag_priorities))
    tag_with_priority.sort(key=lambda x: x[1], reverse=True)
    tags_kr_sorted = [tag for tag, _ in tag_with_priority]
    primary_tag = tags_kr_sorted[0] if tags_kr_sorted else "ê¸°íƒ€"

    processed_data = {
        'rj_code': rj_code,
        'title_jp': title_jp,
        'title_kr': translated_title or title_jp or rj_code,
        'primary_tag': primary_tag,
        'tags_jp': tags_jp,
        'tags': tags_kr_sorted,
        'release_date': item.get('release_date', 'N/A'),
        'thumbnail_url': item.get('thumbnail_url', ''),
        'rating': item.get('rating', 0.0),
        'link': item.get('link', ''),
        'platform': item.get('platform', 'rj'),
        'maker': item.get('maker', ''),
        'timestamp': time.time()
    }

    cache_data('rj', rj_code, processed_data)
    logger.info(f"Processed RJ item: {rj_code}, title_kr={processed_data['title_kr']}")
    return processed_data


# Steam ë°ì´í„° ì²˜ë¦¬
def process_steam_item(identifier):
    cached = get_cached_data('steam', identifier)
    if cached:
        return cached
    data = {
        'title': identifier,
        'title_kr': identifier,
        'primary_tag': "ê¸°íƒ€",
        'tags': ["ê¸°íƒ€"],
        'thumbnail_url': '',
        'platform': 'steam',
        'timestamp': time.time()
    }
    cache_data('steam', identifier, data)
    return data

# ê²Œì„ ë°ì´í„° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@app.route('/games', methods=['POST'])
def process_games():
    try:
        data = request.get_json()
        logger.info(f"Received request with data: {json.dumps(data, ensure_ascii=False)[:1000]}")
        items = data.get('items', [])
        logger.info(f"Processing {len(items)} items")

        results = []
        missing = []

        if not items:
            return jsonify({'results': [], 'missing': [], 'task_id': 'none'})

        # ë¬¸ìì—´ ë°°ì—´ (ìºì‹œ í™•ì¸) ë˜ëŠ” ê°ì²´ ë°°ì—´ (í¬ë¡¤ë§ ë°ì´í„°) ì²˜ë¦¬
        if isinstance(items[0], str):
            for item in items:
                rj_match = re.match(r'^[Rr][Jj]\d{6,8}$', item, re.IGNORECASE)
                if rj_match:
                    rj_code = rj_match.group(0).upper()
                    cached = get_cached_data('rj', rj_code)
                    if cached is not None:
                        # â— error í¬í•¨ë˜ì–´ë„ ë¬´ì¡°ê±´ append, missingì—ëŠ” ë„£ì§€ ì•ŠìŒ
                        results.append(cached)
                    else:
                        missing.append(rj_code)
                        results.append({'error': f'Game not found for {rj_code}', 'platform': 'rj', 'rj_code': rj_code})

                else:
                    results.append(process_steam_item(item))
        else:
            for item in items:
                rj_code = item.get('rj_code')
                if rj_code and re.match(r'^[Rr][Jj]\d{6,8}$', rj_code, re.IGNORECASE) and 'error' not in item:
                    results.append(process_rj_item(item))
                elif 'error' in item:
                    results.append(item)
                else:
                    results.append(process_steam_item(item.get('title', item)))

        task_id = request.headers.get('X-Cloud-Trace-Context', 'manual_task')[:36]
        logger.info(f"Returning response for task_id: {task_id}, results: {len(results)}")
        return jsonify({'results': results, 'missing': missing, 'task_id': task_id})
    except Exception as e:
        logger.error(f"Error processing games: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

# ì§„í–‰ ìƒí™© ì—”ë“œí¬ì¸íŠ¸
@app.route('/progress/<task_id>', methods=['GET'])
def get_progress(task_id):
    try:
        logger.info(f"Progress request for task_id: {task_id}")
        return jsonify({'completed': 0, 'total': 1, 'status': 'completed'})
    except Exception as e:
        logger.error(f"Progress error for task {task_id}: {e}")
        return jsonify({'error': str(e)}), 500
@app.route("/sync-tags", methods=["POST"])
def sync_tags_to_games():
    try:
        logger.info("Starting tag re-sync for all games...")

        # 1. íƒœê·¸ ë³€í™˜ í…Œì´ë¸” ìƒì„±
        tag_map = {
            doc.id: doc.to_dict().get("tag_kr", doc.id)
            for doc in db.collection("tags").document("jp_to_kr").collection("mappings").stream()
        }
        tag_priority = {
            doc.id: doc.to_dict().get("priority", 10)
            for doc in db.collection("tags").document("jp_to_kr").collection("mappings").stream()
        }

        # 2. RJ ê²Œì„ ë¬¸ì„œë“¤ ì—…ë°ì´íŠ¸
        games_ref = db.collection("games").document("rj").collection("items")
        updated_count = 0

        for doc in games_ref.stream():
            game = doc.to_dict()
            tags_jp = game.get("tags_jp", [])
            if not tags_jp:
                continue

            tags_kr = [tag_map.get(jp, "ê¸°íƒ€") for jp in tags_jp]
            primary_tag = max(tags_kr, key=lambda t: tag_priority.get(t, 0), default="ê¸°íƒ€")

            games_ref.document(doc.id).update({
                "tags": tags_kr,
                "primary_tag": primary_tag
            })
            updated_count += 1

        logger.info(f"Completed tag sync. Updated {updated_count} documents.")
        return jsonify({"updated": updated_count})

    except Exception as e:
        logger.error(f"/sync-tags error: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500
    
@app.route('/reorder-tags', methods=['POST'])
def reorder_tags():
    try:
        logger.info("ğŸ”§ íƒœê·¸ ì¬ì •ë ¬ ì‘ì—… ì‹œì‘")

        # âœ… íƒœê·¸ ìš°ì„ ìˆœìœ„ ë¡œë“œ
        tag_priority = {
            doc.id: doc.to_dict().get("priority", 10)
            for doc in db.collection("tags").document("jp_to_kr").collection("mappings").stream()
        }

        logger.info(f"âœ… {len(tag_priority)}ê°œì˜ íƒœê·¸ ìš°ì„ ìˆœìœ„ ë¡œë”© ì™„ë£Œ")

        # ğŸ”„ ì „ì²´ ê²Œì„ ìˆœíšŒ
        games_ref = db.collection("games").document("rj").collection("items")
        updated = 0
        for doc in games_ref.stream():
            data = doc.to_dict()
            tags = data.get("tags", [])
            if not tags:
                continue

            original_tags = tags[:]
            # ğŸ”½ ìš°ì„ ìˆœìœ„ ì •ë ¬
            sorted_tags = sorted(tags, key=lambda t: -tag_priority.get(t, 10))  # âœ… ë†’ì€ ì ìˆ˜ ìš°ì„ 
            primary_tag = sorted_tags[0] if sorted_tags else "ê¸°íƒ€"

            # âœ… ë³€ê²½ì‚¬í•­ ìˆì„ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
            if sorted_tags != tags or primary_tag != data.get("primary_tag"):
                doc.reference.update({
                    "tags": sorted_tags,
                    "primary_tag": primary_tag
                })
                logger.info(f"[UPDATED] {doc.id} : {original_tags} â†’ {sorted_tags}")
                updated += 1

        logger.info(f"ğŸŸ¢ íƒœê·¸ ì¬ì •ë ¬ ì™„ë£Œ: {updated}ê°œ ë¬¸ì„œ ì—…ë°ì´íŠ¸ë¨")
        return jsonify({"status": "ok", "updated_documents": updated})
    except Exception as e:
        logger.error(f"âŒ reorder_tags ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    # GCPì—ì„œë§Œ ì‹¤í–‰
    if os.getenv('GAE_ENV', '').startswith('standard') or os.getenv('CLOUD_RUN', '') == 'true':
        app.run(host='0.0.0.0', port=int(os.getenv('PORT', 8080)))
    else:
        logger.warning("This script should only run in GCP environment. Exiting.")