import json
import logging
import os
import time
import re
from flask import Flask, request, jsonify
from google.cloud import firestore
from google.cloud import storage
from openai import OpenAI

app = Flask(__name__)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("app.log", encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# Firestore í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    db = firestore.Client()
    logger.info("Firestore í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    logger.error(f"Firestore ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    db = None

# GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    gcs_client = storage.Client()
    bucket_name = os.getenv("GCS_BUCKET_NAME", "rjcode")
    bucket = gcs_client.bucket(bucket_name)
    logger.info(f"GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ, ë²„í‚·: {bucket_name}")
except Exception as e:
    logger.error(f"GCS ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    gcs_client = None
    bucket = None

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    logger.error(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    openai_client = None

# ì¼ë³¸ì–´ ê°ì§€ í•¨ìˆ˜
def needs_translation(title: str) -> bool:
    if not title or not isinstance(title, str):
        logger.debug(f"ë²ˆì—­ ë¶ˆí•„ìš”: ì œëª©ì´ ë¹„ì–´ ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {title}")
        return False
    # íˆë¼ê°€ë‚˜(\u3040-\u309F), ê°€íƒ€ì¹´ë‚˜(\u30A0-\u30FF), í•œì(\u4E00-\u9FFF) í¬í•¨ ì—¬ë¶€ í™•ì¸
    has_japanese = bool(re.search(r'[\u3040-\u30FF\u4E00-\u9FFF]', title))
    logger.debug(f"ì¼ë³¸ì–´ ê°ì§€ ê²°ê³¼ '{title}': {'ê°ì§€ë¨' if has_japanese else 'ê°ì§€ë˜ì§€ ì•ŠìŒ'}")
    return has_japanese

# GCS ê²½ë¡œ ìƒì„± í•¨ìˆ˜
def get_gcs_path(platform, rj_code):
    # RJ ì½”ë“œì—ì„œ ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì˜ˆ: RJ123456 -> 123456)
    number_part = rj_code[2:] if rj_code.startswith('RJ') else rj_code
    # ìˆ«ì ë¶€ë¶„ì˜ ì• ë‘ ê¸€ì ì¶”ì¶œ
    prefix = number_part[:2] if len(number_part) >= 2 else number_part.zfill(2)
    return f"{platform}/{prefix}/{rj_code}.json"

# GCSì—ì„œ ìºì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
def get_cached_data(platform, identifier):
    if not bucket:
        logger.warning("GCS ë²„í‚·ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        return None
    rj_code = identifier.upper().replace('-', '').replace('_', '').strip()
    blob_path = get_gcs_path(platform, rj_code)
    blob = bucket.blob(blob_path)

    if blob.exists():
        content = blob.download_as_text()
        data = json.loads(content)

        # âœ… 404 í˜¹ì€ ì˜¤ë¥˜ ìƒíƒœë©´ ë°”ë¡œ ë¦¬í„´
        if data.get("status") == "404" or data.get("permanent_error"):
            logger.info(f"[GCS ìºì‹œ] 404 í™•ì¸: {platform}:{rj_code}")
            return data

        # âœ… íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” ê²½ìš° ë¬´íš¨
        if not data.get("timestamp"):
            logger.warning(f"[GCS ìºì‹œ] íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ: {platform}:{rj_code}")
            return None

        logger.info(f"[GCS ìºì‹œ] ì¡°íšŒ ì„±ê³µ: {platform}:{rj_code}")
        return data
    return None

# GCSì— ìºì‹œ ì €ì¥
def cache_data(platform, rj_code, data):
    if not bucket:
        logger.warning("GCS ë²„í‚·ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        return
    try:
        blob_path = get_gcs_path(platform, rj_code)
        blob = bucket.blob(blob_path)
        blob.upload_from_string(json.dumps(data, ensure_ascii=False), content_type='application/json')
        logger.info(f"[GCS ìºì‹œ] ì €ì¥ ì™„ë£Œ: {blob_path}")
    except Exception as e:
        logger.error(f"[GCS ìºì‹œ ì˜¤ë¥˜] ì €ì¥ ì‹¤íŒ¨: {platform}/{rj_code}, ì˜¤ë¥˜: {e}", exc_info=True)

# íƒœê·¸ ìºì‹œ
def get_cached_tag(tag_jp):
    if not db:
        return None
    try:
        safe_tag_id = normalize_tag_id(tag_jp)
        doc_ref = db.collection('tags').document('jp_to_kr').collection('mappings').document(safe_tag_id)
        doc = doc_ref.get()
        if doc.exists:
            return doc.to_dict()
        return None
    except Exception as e:
        logger.error(f"íƒœê·¸ ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {tag_jp}: {e}")
        return None

def normalize_tag_id(tag_jp):
    # Firestore ë¬¸ì„œ IDë¡œ ì“¸ ìˆ˜ ìˆë„ë¡ ìŠ¬ë˜ì‹œ ì œê±° ë˜ëŠ” ëŒ€ì²´
    return tag_jp.replace("/", "-")

def cache_tag(tag_jp, tag_kr, priority):
    if not db:
        return
    try:
        safe_tag_id = normalize_tag_id(tag_jp)
        normalized_tag_kr = normalize_tag_id(tag_kr)  # ğŸ”¥ í•˜ì´í”ˆ ë“±ìœ¼ë¡œ ì •ì œ
        doc_ref = db.collection('tags').document('jp_to_kr').collection('mappings').document(safe_tag_id)
        doc_ref.set({
            'tag_jp': tag_jp,        # ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥
            'tag_kr': normalized_tag_kr,
            'priority': priority
        })
        logger.info(f"íƒœê·¸ ìºì‹œ ì €ì¥: {tag_jp} â†’ {normalized_tag_kr} (ID: {safe_tag_id})")
    except Exception as e:
        logger.error(f"íƒœê·¸ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {tag_jp}: {e}")

# GPT ë²ˆì—­
def translate_with_gpt_batch(tags, title_jp=None, batch_idx=""):
    if not openai_client:
        logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        return tags, title_jp
    try:
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        prompt = (
            "ë‹¹ì‹ ì€ ì¼ë³¸ì–´ì—ì„œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ì˜ íƒœê·¸ë“¤ê³¼ ì œëª©ì„ ë¬¸ë§¥ì— ë§ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë©°, ì˜ˆì‹œì™€ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì œëª©(title)ë²ˆì—­ì€ í•œêµ­ì–´ë‚˜ ì˜ì–´ì¸ ê²½ìš°ë§Œ ìƒëµí•´ë„ ë©ë‹ˆë‹¤.\n\n"
            "ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\n"
            "{\n"
            "  \"tags\": [\"ë²ˆì—­ëœíƒœê·¸1\", \"ë²ˆì—­ëœíƒœê·¸2\", ...],\n"
            "  \"title\": \"ë²ˆì—­ëœì œëª©\"\n"
            "}\n\n"
            "ì˜ˆì‹œ ì…ë ¥/ì¶œë ¥:\n"
            "Input:\n"
            "Tags: RPG, ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n"
            "Title: å°‘å¥³ã®å†’é™º\n"
            "Output:\n"
            "{\n"
            "  \"tags\": [\"RPG\", \"ì•¡ì…˜\"],\n"
            "  \"title\": \"ì†Œë…€ì˜ ëª¨í—˜\"\n"
            "}\n\n"
            f"Input:\n"
            f"Tags: {', '.join(tags) if tags else 'ì—†ìŒ'}\n"
            f"Title: {title_jp if title_jp else 'ì—†ìŒ'}\n"
            "Output:"
        )

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a translator specializing in Japanese to Korean."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200
        )
        response_text = response.choices[0].message.content.strip()
        logger.debug(f"GPT ì‘ë‹µ: {batch_idx}: {response_text}")

        # JSON íŒŒì‹±
        try:
            response_json = json.loads(response_text)
            translated_tags = response_json.get('tags', tags)
            translated_title = response_json.get('title', title_jp) if title_jp else title_jp
        except json.JSONDecodeError:
            logger.warning(f"ì˜ëª»ëœ JSON ì‘ë‹µ: {batch_idx}: {response_text}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ íŒŒì‹±
            parts = response_text.split(';')
            translated_tags = [t.strip() for t in parts[0].split(',')][:len(tags)]
            translated_title = parts[1].strip() if len(parts) > 1 and title_jp else title_jp

        # ë²ˆì—­ ì‹¤íŒ¨ ê°ì§€
        if title_jp and translated_title and needs_translation(translated_title):
            logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {batch_idx}: translated_title={translated_title}ì€ ì—¬ì „íˆ ì¼ë³¸ì–´")
            translated_title = title_jp  # ì¼ë³¸ì–´ë¡œ ë°˜í™˜ëœ ê²½ìš° ì›ë³¸ ìœ ì§€

        logger.info(f"ë²ˆì—­ ì™„ë£Œ: {batch_idx}: tags={translated_tags}, title={translated_title}")
        return translated_tags, translated_title
    except Exception as e:
        logger.error(f"GPT ë²ˆì—­ ì˜¤ë¥˜: ë°°ì¹˜ {batch_idx}: {e}")
        return tags, title_jp  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë˜ ì œëª© ìœ ì§€

# RJ ë°ì´í„° ì²˜ë¦¬
def process_rj_item(item):
    if 'error' in item:
        rj_code = item.get('rj_code') or item.get('title') or item.get('original') or 'unknown'
        if not re.match(r'^RJ\d{6,8}$', rj_code, re.IGNORECASE):
            logger.warning(f"[ì˜¤ë¥˜ í•­ëª©] ìœ íš¨í•˜ì§€ ì•Šì€ RJ ì½”ë“œ, ì €ì¥ ìƒëµ: {rj_code}")
            return {
                'rj_code': rj_code,
                'error': item.get('error'),
                'platform': 'rj',
                'timestamp': time.time()
            }
        error_data = {
            'rj_code': rj_code,
            'platform': 'rj',
            'title': "ì•Œ ìˆ˜ ì—†ìŒ",
            'title_kr': "ì•Œ ìˆ˜ ì—†ìŒ",
            'title_jp': "ä¸æ˜",
            'tags': ["ê¸°íƒ€"],
            'tags_jp': ["ãã®ä»–"],
            'primary_tag': "ê¸°íƒ€",
            'error': item.get('error'),
            'timestamp': time.time()
        }
        logger.warning(f"[ì˜¤ë¥˜ í•­ëª©] ìºì‹œì— ì €ì¥: {rj_code}")
        cache_data('rj', rj_code, error_data)
        return error_data

    rj_code = item.get('rj_code')
    cached = get_cached_data('rj', rj_code)
    if cached and cached.get('title_kr') and not needs_translation(cached.get('title_kr')):
        logger.debug(f"ìºì‹œ ë°ì´í„° ì‚¬ìš©: {rj_code}: title_kr={cached.get('title_kr')}")
        return cached

    tags_jp = item.get('tags_jp', [])
    tags_jp = [tag.strip() for tag in tags_jp]  # ê³µë°± ì œê±°
    tags_jp = [normalize_tag_id(tag) for tag in tags_jp]
    title_jp = item.get('title_jp', '')
    tags_kr = []
    tags_to_translate = []
    tag_priorities = []

    # RJ ì½”ë“œ ì œê±° í•¨ìˆ˜
    def clean_title(title, rj_code):
        if not title or not rj_code:
            return title
        patterns = [
            rf"[\[\(]?\b{rj_code}\b[\]\)]?[)\s,;ï¼š]*",
            rf"[ _\-]?\bRJ\s*{rj_code[2:]}\b",
            rf"\b{rj_code}\b"
        ]
        cleaned = title
        for pattern in patterns:
            cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE).strip()
        return cleaned

    # title_jp ì •ì œ
    cleaned_title_jp = clean_title(title_jp, rj_code)

    for tag in tags_jp:
        cached_tag = get_cached_tag(tag)
        if cached_tag:
            kr = cached_tag['tag_kr']
            priority = cached_tag.get('priority', 10)
            tags_kr.append(kr)
            tag_priorities.append(priority)
        else:
            tags_to_translate.append(tag)
            tag_priorities.append(10)

    translated_tags = tags_jp
    translated_title = cleaned_title_jp

    if needs_translation(cleaned_title_jp) or tags_to_translate:
        logger.debug(f"ë²ˆì—­ ìš”ì²­: {rj_code}: title_jp={cleaned_title_jp}, tags={tags_to_translate}")
        translated_tags, translated_title = translate_with_gpt_batch(
            tags_to_translate,
            cleaned_title_jp if needs_translation(cleaned_title_jp) else None,
            batch_idx=rj_code
        )
        # ë²ˆì—­ëœ ì œëª©ë„ RJ ì½”ë“œ ì œê±°
        translated_title = clean_title(translated_title or cleaned_title_jp, rj_code)
        for i, (jp, kr) in enumerate(zip(tags_to_translate, translated_tags)):
            existing = get_cached_tag(jp)
            priority = existing.get("priority", 10) if existing else 10
            tags_kr.append(kr)
            tag_priorities.append(priority)
            cache_tag(jp, kr, priority)
    else:
        logger.debug(f"ë²ˆì—­ ë¶ˆí•„ìš”: {rj_code}: title_jp={cleaned_title_jp}")

    tag_with_priority = list(zip(tags_kr, tag_priorities))
    tag_with_priority.sort(key=lambda x: x[1], reverse=True)
    tags_kr_sorted = [tag for tag, _ in tag_with_priority]
    primary_tag = tags_kr_sorted[0] if tags_kr_sorted else "ê¸°íƒ€"

    processed_data = {
        'rj_code': rj_code,
        'title_jp': cleaned_title_jp,
        'title_kr': translated_title or cleaned_title_jp or rj_code,
        'primary_tag': primary_tag,
        'tags_jp': tags_jp,
        'tags': tags_kr_sorted,
        'release_date': item.get('release_date', 'N/A'),
        'thumbnail_url': item.get('thumbnail_url', ''),
        'rating': item.get('rating', 0.0),
        'link': item.get('link', ''),
        'platform': 'rj',
        'maker': item.get('maker', ''),
        'timestamp': time.time()
    }

    cache_data('rj', rj_code, processed_data)
    logger.info(f"RJ í•­ëª© ì²˜ë¦¬ ì™„ë£Œ: {rj_code}, title_kr={processed_data['title_kr']}")
    return processed_data

# Steam ë°ì´í„° ì²˜ë¦¬
def process_steam_item(identifier):
    return {
        'title': identifier,
        'title_kr': identifier,
        'primary_tag': "ê¸°íƒ€",
        'tags': ["ê¸°íƒ€"],
        'thumbnail_url': '',
        'platform': 'ê¸°íƒ€'  # ğŸ”¥ platformì„ "steam" â†’ "ê¸°íƒ€"ë¡œ ë³€ê²½
        # 'timestamp': time.time()
    }

# ê²Œì„ ë°ì´í„° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@app.route('/games', methods=['POST'])
def process_games():
    try:
        data = request.get_json()
        logger.info(f"ìš”ì²­ ë°ì´í„° ìˆ˜ì‹ : {json.dumps(data, ensure_ascii=False)[:1000]}")
        items = data.get('items', [])
        logger.info(f"{len(items)}ê°œ í•­ëª© ì²˜ë¦¬ ì‹œì‘")

        results = []
        missing = []

        if not items:
            return jsonify({'results': [], 'missing': [], 'task_id': 'none'})

        for item in items:
            # ë¬¸ìì—´(RJ ì½”ë“œ)ë§Œ ë°›ì€ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            if isinstance(item, str):
                # RJ ì½”ë“œ íŒ¨í„´ í™•ì¸
                if re.match(r'^RJ\d{6,8}$', item, re.IGNORECASE):
                    item = {
                        "rj_code": item.upper(),
                        "platform": "rj"
                    }
                    logger.info(f"[ë¬¸ìì—´ ë³€í™˜] {item['rj_code']}")
                else:
                    item = {
                        "title": item,
                        "platform": "steam"
                    }
                    logger.info(f"[ë¬¸ìì—´ ë³€í™˜] Steam ì œëª©: {item['title']}")

            # ì´ì œ itemì€ í™•ì‹¤íˆ ë”•ì…”ë„ˆë¦¬ íƒ€ì…
            logger.info(f"[í•­ëª© ì²˜ë¦¬] {json.dumps(item, ensure_ascii=False)}")

            # ìºì‹œ ì €ì¥ ìš”ì²­ì¼ ê²½ìš° (í¬ë¡¤ë§ ì„±ê³µ or ì‹¤íŒ¨ í›„)
            if isinstance(item, dict) and item.get("timestamp"):
                platform = item.get("platform", "rj")
                rj_code = item.get("rj_code")
                title = item.get("title_kr") or item.get("title") or rj_code

                # skip_translation í”Œë˜ê·¸ ë˜ëŠ” 404 ìƒíƒœì´ë©´ ë²ˆì—­ ì—†ì´ ë°”ë¡œ ì²˜ë¦¬
                if item.get("skip_translation") or item.get("status") == "404" or item.get("permanent_error"):
                    logger.info(f"[ì§ì ‘ ì €ì¥] {platform}:{rj_code}")
                    processed = process_and_save_rj_item(item)  # ì´ë¯¸ ë²ˆì—­ ìŠ¤í‚µ ë¡œì§ì´ í¬í•¨ë¨
                    results.append(processed)
                # ê¸°ì¡´ ë²ˆì—­/ì €ì¥ ì¡°ê±´
                elif platform == "rj" and (not item.get("title_kr") or not item.get("tags")):
                    logger.info(f"[ë²ˆì—­ ë° ì €ì¥] {platform}:{rj_code}")
                    processed = process_and_save_rj_item(item)
                    results.append(processed)
                else:
                    cache_data(platform, rj_code, item)
                    logger.info(f"[ì €ì¥ ì™„ë£Œ] {platform}/items/{rj_code}, title_kr={title}")
                    results.append(item)

            # ìºì‹œ í™•ì¸ ìš”ì²­ì¼ ê²½ìš°
            else:
                rj_code = item.get("rj_code") if isinstance(item, dict) else None
                platform = item.get("platform", "rj") if isinstance(item, dict) else "rj"

                # RJ ì—†ëŠ” ê²½ìš° steam ì²˜ë¦¬
                if not rj_code:
                    title = item.get("title", "untitled") if isinstance(item, dict) else str(item)
                    steam_fallback = process_steam_item(title)
                    logger.info(f"[Steam ëª¨ë“œ] ì œëª©={steam_fallback.get('title')}")
                    results.append(steam_fallback)
                    continue

                # ìºì‹œ í™•ì¸
                cached = get_cached_data(platform, rj_code)
                if cached and cached.get("timestamp"):
                    logger.info(f"[ìºì‹œ ì¡°íšŒ ì„±ê³µ] {platform}:{rj_code}")
                    results.append(cached)
                else:
                    logger.info(f"[ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨] {platform}:{rj_code}")
                    missing.append(rj_code)

        task_id = request.headers.get('X-Cloud-Trace-Context', 'manual_task')[:36]
        logger.info(f"ì‘ë‹µ ë°˜í™˜: task_id={task_id}, ê²°ê³¼={len(results)}, ëˆ„ë½={len(missing)}")
        return jsonify({'results': results, 'missing': missing, 'task_id': task_id})

    except Exception as e:
        logger.error(f"ê²Œì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

# ì§„í–‰ ìƒí™© ì—”ë“œí¬ì¸íŠ¸
@app.route('/progress/<task_id>', methods=['GET'])
def get_progress(task_id):
    try:
        logger.info(f"ì§„í–‰ ìƒí™© ìš”ì²­: task_id={task_id}")
        return jsonify({'completed': 0, 'total': 1, 'status': 'completed'})
    except Exception as e:
        logger.error(f"ì§„í–‰ ìƒí™© ì¡°íšŒ ì˜¤ë¥˜: task_id={task_id}: {e}")
        return jsonify({'error': str(e)}), 500

@app.route("/sync-tags", methods=["POST"])
def sync_tags_to_games():
    try:
        logger.info("ëª¨ë“  ê²Œì„ì˜ íƒœê·¸ ë™ê¸°í™” ì‹œì‘")

        # 1. íƒœê·¸ ë³€í™˜ í…Œì´ë¸” ìƒì„±
        tag_map = {
            doc.id: doc.to_dict().get("tag_kr", doc.id)
            for doc in db.collection("tags").document("jp_to_kr").collection("mappings").stream()
        }
        tag_priority = {
            doc.id: doc.to_dict().get("priority", 10)
            for doc in db.collection("tags").document("jp_to_kr").collection("mappings").stream()
        }

        # 2. RJ ê²Œì„ ë¬¸ì„œë“¤ ì—…ë°ì´íŠ¸
        games_ref = db.collection("games").document("rj").collection("items")
        updated_count = 0

        for doc in games_ref.stream():
            game = doc.to_dict()
            tags_jp = game.get("tags_jp", [])
            if not tags_jp:
                continue

            tags_kr = [tag_map.get(jp, "ê¸°íƒ€") for jp in tags_jp]
            primary_tag = max(tags_kr, key=lambda t: tag_priority.get(t, 0), default="ê¸°íƒ€")

            games_ref.document(doc.id).update({
                "tags": tags_kr,
                "primary_tag": primary_tag
            })
            updated_count += 1

        logger.info(f"íƒœê·¸ ë™ê¸°í™” ì™„ë£Œ: {updated_count}ê°œ ë¬¸ì„œ ì—…ë°ì´íŠ¸")
        return jsonify({"updated": updated_count})

    except Exception as e:
        logger.error(f"íƒœê·¸ ë™ê¸°í™” ì˜¤ë¥˜: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/reorder-tags', methods=['POST'])
def reorder_tags():
    try:
        logger.info("íƒœê·¸ ì¬ì •ë ¬ ì‘ì—… ì‹œì‘")

        # âœ… íƒœê·¸ ìš°ì„ ìˆœìœ„ ë¡œë“œ
        tag_priority = {
            doc.id: doc.to_dict().get("priority", 10)
            for doc in db.collection("tags").document("jp_to_kr").collection("mappings").stream()
        }

        logger.info(f"{len(tag_priority)}ê°œì˜ íƒœê·¸ ìš°ì„ ìˆœìœ„ ë¡œë”© ì™„ë£Œ")

        # ğŸ”„ ì „ì²´ ê²Œì„ ìˆœíšŒ
        games_ref = db.collection("games").document("rj").collection("items")
        updated = 0
        for doc in games_ref.stream():
            data = doc.to_dict()
            tags = data.get("tags", [])
            if not tags:
                continue

            original_tags = tags[:]
            # ğŸ”½ ìš°ì„ ìˆœìœ„ ì •ë ¬
            sorted_tags = sorted(tags, key=lambda t: -tag_priority.get(t, 10))  # âœ… ë†’ì€ ì ìˆ˜ ìš°ì„ 
            primary_tag = sorted_tags[0] if sorted_tags else "ê¸°íƒ€"

            # âœ… ë³€ê²½ì‚¬í•­ ìˆì„ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
            if sorted_tags != tags or primary_tag != data.get("primary_tag"):
                doc.reference.update({
                    "tags": sorted_tags,
                    "primary_tag": primary_tag
                })
                logger.info(f"[ì—…ë°ì´íŠ¸] {doc.id}: {original_tags} â†’ {sorted_tags}")
                updated += 1

        logger.info(f"íƒœê·¸ ì¬ì •ë ¬ ì™„ë£Œ: {updated}ê°œ ë¬¸ì„œ ì—…ë°ì´íŠ¸")
        return jsonify({"status": "ok", "updated_documents": updated})
    except Exception as e:
        logger.error(f"íƒœê·¸ ì¬ì •ë ¬ ì˜¤ë¥˜: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

def process_and_save_rj_item(item):
    """ë²ˆì—­ë˜ì§€ ì•Šì€ RJ í•­ëª©ì„ ì²˜ë¦¬í•˜ê³  ì €ì¥"""
    rj_code = item.get("rj_code", "unknown")
    
    # ë²ˆì—­ ìŠ¤í‚µ í”Œë˜ê·¸ í™•ì¸
    if item.get("skip_translation") or item.get("status") == "404" or item.get("permanent_error"):
        logger.info(f"[ë²ˆì—­ ìŠ¤í‚µ] {rj_code}: ë²ˆì—­ ì—†ì´ ë°”ë¡œ ì €ì¥")
        # title_krì´ ì—†ìœ¼ë©´ original ë˜ëŠ” title í•„ë“œë¥¼ ì‚¬ìš©
        if not item.get("title_kr"):
            original_name = item.get("original") or item.get("title") or ""
            item["title_kr"] = clean_rj_code(original_name, rj_code)
        
        # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not item.get("tags"):
            item["tags"] = ["ê¸°íƒ€"]
            item["primary_tag"] = "ê¸°íƒ€"
            
        cache_data("rj", rj_code, item)
        return item
    
    title_jp = item.get("title_jp", "")
    tags_jp = item.get("tags_jp", [])
    tags_jp = [normalize_tag_id(tag) for tag in tags_jp]

    if not title_jp and not tags_jp:
        logger.warning(f"[ë¶ˆì™„ì „ í•­ëª©] {rj_code}: title_jp/tags_jp ì—†ìŒ")
        return item

    # title ì •ì œ
    def clean_title(title, rj_code):
        if not title or not rj_code:
            return title
        patterns = [
            rf"[\[\(]?\b{rj_code}\b[\]\)]?[)\s,;ï¼š]*",
            rf"[ _\-]?\bRJ\s*{rj_code[2:]}\b",
            rf"\b{rj_code}\b"
        ]
        for pattern in patterns:
            title = re.sub(pattern, "", title, flags=re.IGNORECASE).strip()
        return title

    cleaned_title = clean_title(title_jp, rj_code)

    # íƒœê·¸ ìºì‹± í™•ì¸ ë° ë²ˆì—­í•  ëª©ë¡ ì¶”ë¦¼
    tags_kr = []
    tags_to_translate = []
    priorities = []

    for tag in tags_jp:
        cached_tag = get_cached_tag(tag)
        if cached_tag:
            tags_kr.append(cached_tag['tag_kr'])
            priorities.append(cached_tag.get('priority', 10))
        else:
            tags_to_translate.append(tag)
            priorities.append(10)

    # ë²ˆì—­
    translated_tags, translated_title = translate_with_gpt_batch(
        tags_to_translate, cleaned_title if needs_translation(cleaned_title) else None, batch_idx=rj_code
    )

    # ìºì‹±
    for i, jp_tag in enumerate(tags_to_translate):
        kr_tag = translated_tags[i]
        cache_tag(jp_tag, kr_tag, priorities[i])
        tags_kr.append(kr_tag)

    # ìš°ì„ ìˆœìœ„ ì •ë ¬
    tag_with_priority = list(zip(tags_kr, priorities))
    tag_with_priority.sort(key=lambda x: x[1], reverse=True)
    tags_kr_sorted = [tag for tag, _ in tag_with_priority]
    primary_tag = tags_kr_sorted[0] if tags_kr_sorted else "ê¸°íƒ€"

    # ìµœì¢… ë°ì´í„° êµ¬ì„±
    final = {
        **item,
        "title_kr": translated_title or cleaned_title or title_jp,
        "tags": tags_kr_sorted,
        "primary_tag": primary_tag,
        "timestamp": time.time()
    }

    # ì €ì¥
    cache_data("rj", rj_code, final)
    logger.info(f"[ìë™ ì €ì¥] {rj_code} â†’ {final['title_kr']}")
    return final

@app.route('/check_permanent_failure/<rj_code>', methods=['GET'])
def check_failure(rj_code):
    try:
        doc = db.collection("games").document("rj").collection("items").document(rj_code).get()
        if doc.exists:
            data = doc.to_dict()
            return jsonify({
                "permanent_failure": data.get("status") == "404" or data.get("permanent_error") == True
            })
        return jsonify({"permanent_failure": False})
    except Exception as e:
        logger.error(f"ì‹¤íŒ¨ í™•ì¸ ì˜¤ë¥˜: {e}")
        return jsonify({"permanent_failure": False})

if __name__ == '__main__':
    # GCPì—ì„œë§Œ ì‹¤í–‰
    if os.getenv('GAE_ENV', '').startswith('standard') or os.getenv('CLOUD_RUN', '') == 'true':
        app.run(host='0.0.0.0', port=int(os.getenv('PORT', 8080)))
    else:
        logger.warning("ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” GCP í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")